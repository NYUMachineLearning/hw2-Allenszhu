---
title: "Regression"
author: "Allen Zhu"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

# Lab Section

In this lab, we will go over regression. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html`



```{r}
library(caret)
library(tidyverse)
library(glmnet)
library(elasticnet)

```
# Perfomance Metrics 

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$


\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. 

3. See how your model did on the training data.

4. Test how your model performs on the test data. 

# Regression

```{r, include=FALSE}
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(ggfortify)


#Mauna Loa CO2 concentrations
data(airquality)
```


1. Split data into training and test set (75% in train set, 25% in test set)

```{r}

## 75% of the sample size
smp_size <- floor(0.75 * nrow(airquality))

set.seed(15)

train_aq <- sample(seq_len(nrow(airquality)), size = smp_size)

train <- airquality[train_aq, ]
test <- airquality[-train_aq, ]


train_regression <- airquality[train_aq, ] 
test_regression <- airquality[-train_aq, ] 

```


### Linear Regression

* Assumes a linear relationship. 
* Independent variables should not be correlated (no mulitcollinearity)
* The number of observations should be greater than the number of independent variables.


$$RSS=\sum(y_i - \hat{y_i})^2$$
We will predict the response of the Temperature based on Wind. 

This is the data we will fit a linear model to. 
```{r}
ggplot(data = train_regression) +
  geom_point(aes(x=Wind, y=Temp)) +
  theme_bw()
```

2. Create and fit a linear model to predict Temperature from Wind using the training set

```{r}
#help(train)

linear_regression <- train(Temp ~ Wind, data = train_regression, method = "lm")

linear_regression$finalModel
```


3. Vizualize how your model performed on the train data by plotting the regression line on top of the train data points. 
```{r}
ggplot(data = train_regression) +
  geom_point(aes(x=Wind, y=Temp), col = 'blue') +
  geom_abline(aes(intercept = 91.969, slope = -1.498), col = 'green') +
  theme_bw()

```


4. Explore how the model performs on the test data. For Linear Regression:

* The residuals should be close to zero.
* There should be equal variance around the regression line (homoscedasticity).
* Residuals should be normally distributed.
* Independent variables and residuals should not be correlated.

4 a) See how the model performs on the test data
```{r}
#help(predict)
linear_predict <- predict(linear_regression, newdata=test_regression)
Predicted_Temp <- linear_predict
comp_df <- cbind(test_regression, Predicted_Temp)
comp_df


```

4 b) Look at the residuals. Are they close to zero?

It seems that residuals start closing in on zero when approaching the medium. The fact that the absolute values of 1Q and 3Q are similar is also important in showing that the data is at least somewhat normally distributed.
```{r}
#look at the median residual value. Close to zero is best
# help(summary)
summary(linear_regression)

```


4 c) Plot predicted temperature vs observed temperature. A strong model should show a strong correlation
```{r}
temp_predictor <- train(Temp ~ Predicted_Temp, data = comp_df, method = "lm")
summary(temp_predictor)

comp_df

ggplot(data = comp_df) +
  geom_point(aes(x=Predicted_Temp, y=Temp), col = 'blue') +
  geom_abline(aes(intercept = 44.83, slope = 0.4551), col = 'green') 
```

4 d) Visualize the predicted values in relation to the real data points. Look for homoscedasticity

The variance does not seem to increase
```{r}
# Extract coefficients from the model

# plot the regression line on the predicted values

# plot the original test values
summary(linear_regression)$coefficients
coefficients(linear_regression)

ggplot(data = comp_df) +
  geom_point(aes(x = Wind, y = Temp, col = 'Observed Temperatures')) +
  geom_point(aes(x = Wind, y = Predicted_Temp, col = 'Predicted Temperatures')) +
  geom_segment(aes(x=Wind, y=Predicted_Temp, xend = Wind, yend = Temp)) +
  geom_abline(aes(intercept = 91.969, slope = -1.498)) +
  theme_bw()

?

linear_regression
```

4 e) Residuals should be normally distributed. Plot the density of the residuals
```{r}

plot(density(resid(linear_regression)), main = 'resid dens')

```


4 f) Independent variables and residuals should not be correlated

As shown below, the correlation value is essentially 0, and have virtually no correlation.
```{r}
cor.test(train_regression$Wind, resid(linear_regression))
```


### Linear Regression with Regularization

5. Create a linear model using L1 or L2 regularization to predict Temperature from Wind and Month variables. Plot your predicted values and the real Y values on the same plot. 
```{r}
temp_predictor2 <- train(Temp ~ Wind + Month, data = train_regression, method = "lasso")


temp_predictor2$finalModel

multilinear_predict <- predict(temp_predictor2, newdata=test_regression)

summary(temp_predictor2)

multi <- test_regression

multi <- cbind(multi, multilinear_predict)
multi
```

```{r}

ggplot(data = multi) +
  geom_point(aes(x=Wind, y=multilinear_predict, col = 'Predicted')) +
  geom_abline(aes(intercept = 91.969, slope = -1.498, col = 'Wind Only lm'))  +
  geom_point(aes(x = Wind, y = Temp, col = 'Observed')) +
  ylab('Predicted Temp')




temp_predictor2$finalModel
```

gaussian",alpha=1)
```